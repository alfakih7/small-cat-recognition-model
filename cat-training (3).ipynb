{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:27:54.327937Z",
     "iopub.status.busy": "2025-04-20T16:27:54.327286Z",
     "iopub.status.idle": "2025-04-20T16:28:06.858830Z",
     "shell.execute_reply": "2025-04-20T16:28:06.858149Z",
     "shell.execute_reply.started": "2025-04-20T16:27:54.327906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install timm albumentations\n",
    "!pip install faiss-cpu  # Use this on Kaggle instead of faiss-gpu\n",
    "!pip install -q timm albumentations\n",
    "!pip install -U albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:28:17.530087Z",
     "iopub.status.busy": "2025-04-20T16:28:17.529777Z",
     "iopub.status.idle": "2025-04-20T16:28:23.719539Z",
     "shell.execute_reply": "2025-04-20T16:28:23.718898Z",
     "shell.execute_reply.started": "2025-04-20T16:28:17.530060Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "from PIL import Image\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "from torchvision import models, transforms\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths\n",
    "TRAIN_DIR = '/kaggle/input/tammathon-task-1/train/train'\n",
    "VAL_DIR = '/kaggle/input/tammathon-task-1/val/val'\n",
    "TEST_DIR = '/kaggle/input/tammathon-task-1/test/test'\n",
    "\n",
    "# Check if directories exist\n",
    "print(f\"Train directory exists: {os.path.exists(TRAIN_DIR)}\")\n",
    "print(f\"Validation directory exists: {os.path.exists(VAL_DIR)}\")\n",
    "print(f\"Test directory exists: {os.path.exists(TEST_DIR)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T16:28:39.026103Z",
     "iopub.status.busy": "2025-04-20T16:28:39.025663Z",
     "iopub.status.idle": "2025-04-20T16:31:28.018169Z",
     "shell.execute_reply": "2025-04-20T16:31:28.017463Z",
     "shell.execute_reply.started": "2025-04-20T16:28:39.026078Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# List all directories (cat IDs)\n",
    "cat_dirs = [d for d in os.listdir(TRAIN_DIR) if os.path.isdir(os.path.join(TRAIN_DIR, d))]\n",
    "total_cats = len(cat_dirs)\n",
    "print(f\"Total number of cat classes: {total_cats}\")\n",
    "\n",
    "# Sample 10,000 cat IDs for initial training instead of using all\n",
    "sample_size = 10000  # Use 10,000 cats instead of all ~110K cats\n",
    "sampled_cat_ids = random.sample(cat_dirs, min(sample_size, total_cats))\n",
    "print(f\"Using {len(sampled_cat_ids)} cat classes for training (sampled from {total_cats} total)\")\n",
    "\n",
    "# Display sample images from a few random classes\n",
    "visualization_cats = random.sample(sampled_cat_ids, 5)  # Just show 5 random cats\n",
    "plt.figure(figsize=(15, 10))\n",
    "for i, cat_id in enumerate(visualization_cats):\n",
    "    cat_path = os.path.join(TRAIN_DIR, cat_id)\n",
    "    images = os.listdir(cat_path)\n",
    "    \n",
    "    for j, img_name in enumerate(images[:2]):  # Show 2 images per cat\n",
    "        if j >= 2:\n",
    "            break\n",
    "            \n",
    "        img_path = os.path.join(cat_path, img_name)\n",
    "        img = Image.open(img_path)\n",
    "        \n",
    "        plt.subplot(5, 2, i*2 + j + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"Cat ID: {cat_id}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Count images per cat (sampling a subset for statistics to save time)\n",
    "print(\"Calculating image statistics (sampling 5000 cats for efficiency)...\")\n",
    "stat_sample = random.sample(sampled_cat_ids, min(5000, len(sampled_cat_ids)))\n",
    "images_per_cat = {}\n",
    "for cat_id in tqdm(stat_sample):\n",
    "    cat_path = os.path.join(TRAIN_DIR, cat_id)\n",
    "    images = os.listdir(cat_path)\n",
    "    images_per_cat[cat_id] = len(images)\n",
    "\n",
    "# Display distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(list(images_per_cat.values()), bins=10)\n",
    "plt.title('Distribution of Images per Cat (Sample of 5000 cats)')\n",
    "plt.xlabel('Number of Images')\n",
    "plt.ylabel('Number of Cats')\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average images per cat: {np.mean(list(images_per_cat.values())):.2f}\")\n",
    "print(f\"Min images per cat: {min(images_per_cat.values())}\")\n",
    "print(f\"Max images per cat: {max(images_per_cat.values())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-20T16:32:08.291088Z",
     "iopub.status.busy": "2025-04-20T16:32:08.290766Z",
     "iopub.status.idle": "2025-04-20T16:32:41.266590Z",
     "shell.execute_reply": "2025-04-20T16:32:41.265997Z",
     "shell.execute_reply.started": "2025-04-20T16:32:08.291064Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define image transformations for training and validation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Define function to find images by cat_id\n",
    "def find_image_by_cat_id(cat_id, directory=TRAIN_DIR):\n",
    "    cat_path = os.path.join(directory, cat_id)\n",
    "    if os.path.isdir(cat_path):\n",
    "        img_files = os.listdir(cat_path)\n",
    "        if img_files:\n",
    "            return os.path.join(cat_path, img_files[0])\n",
    "    \n",
    "    print(f\"No image found for cat_id {cat_id}\")\n",
    "    return None\n",
    "\n",
    "# Dataset class for cat faces\n",
    "class CatFaceDataset(Dataset):\n",
    "    def __init__(self, directory, cat_ids=None, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.samples = []\n",
    "        \n",
    "        # If cat_ids not provided, use all cat IDs in the directory\n",
    "        if cat_ids is None:\n",
    "            cat_ids = [d for d in os.listdir(directory) if os.path.isdir(os.path.join(directory, d))]\n",
    "        \n",
    "        # Create a list of (cat_id, image_path) pairs\n",
    "        for cat_id in cat_ids:\n",
    "            cat_path = os.path.join(directory, cat_id)\n",
    "            if os.path.isdir(cat_path):\n",
    "                for img_name in os.listdir(cat_path):\n",
    "                    img_path = os.path.join(cat_path, img_name)\n",
    "                    self.samples.append((cat_id, img_path))\n",
    "        \n",
    "        print(f\"Loaded {len(self.samples)} images for {len(set([s[0] for s in self.samples]))} cats\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cat_id, img_path = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            # If image can't be loaded, create a blank image\n",
    "            img = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        # Return image, dummy index, and cat_id\n",
    "        return img, idx, cat_id\n",
    "\n",
    "# Triplet batch sampler for triplet learning\n",
    "class TripletBatchSampler(Sampler):\n",
    "    def __init__(self, dataset, batch_size):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        # Group indices by cat_id\n",
    "        self.cat_id_to_indices = {}\n",
    "        for idx, (_, _, cat_id) in enumerate(dataset):\n",
    "            if cat_id not in self.cat_id_to_indices:\n",
    "                self.cat_id_to_indices[cat_id] = []\n",
    "            self.cat_id_to_indices[cat_id].append(idx)\n",
    "        \n",
    "        # Filter cat_ids with at least 2 images (for positive pairs)\n",
    "        self.valid_cat_ids = [cat_id for cat_id, indices in self.cat_id_to_indices.items() \n",
    "                              if len(indices) >= 2]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        # Create triplets\n",
    "        triplets = []\n",
    "        for _ in range(len(self.dataset) // 3):  # Roughly same number of triplets as original dataset size\n",
    "            if not self.valid_cat_ids:\n",
    "                break\n",
    "                \n",
    "            # Select a random cat_id for anchor/positive\n",
    "            anchor_cat_id = random.choice(self.valid_cat_ids)\n",
    "            \n",
    "            # Select two different indices for this cat (anchor and positive)\n",
    "            anchor_pos_indices = random.sample(self.cat_id_to_indices[anchor_cat_id], 2)\n",
    "            \n",
    "            # Select a different cat_id for negative\n",
    "            negative_cat_ids = [cat_id for cat_id in self.valid_cat_ids if cat_id != anchor_cat_id]\n",
    "            if not negative_cat_ids:\n",
    "                continue\n",
    "                \n",
    "            negative_cat_id = random.choice(negative_cat_ids)\n",
    "            \n",
    "            # Select an index for negative\n",
    "            negative_idx = random.choice(self.cat_id_to_indices[negative_cat_id])\n",
    "            \n",
    "            # Add the triplet\n",
    "            triplets.extend([anchor_pos_indices[0], anchor_pos_indices[1], negative_idx])\n",
    "        \n",
    "        # Return triplet indices in batches\n",
    "        for i in range(0, len(triplets), self.batch_size):\n",
    "            yield triplets[i:i + self.batch_size]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return max(1, len(self.dataset) // 3 // self.batch_size)\n",
    "\n",
    "# Create datasets\n",
    "print(\"Creating training dataset...\")\n",
    "train_dataset = CatFaceDataset(TRAIN_DIR, cat_ids=sampled_cat_ids, transform=train_transform)\n",
    "\n",
    "# Split the same cats into train and validation datasets\n",
    "print(\"Creating validation dataset from same cat IDs...\")\n",
    "# Create dictionary to track cat_id -> images mapping\n",
    "cat_to_images = {}\n",
    "for cat_id in sampled_cat_ids:\n",
    "    cat_path = os.path.join(TRAIN_DIR, cat_id)\n",
    "    if os.path.isdir(cat_path):\n",
    "        cat_to_images[cat_id] = [os.path.join(cat_id, img) for img in os.listdir(cat_path)]\n",
    "\n",
    "# Create a validation dataset by sampling 20% of images from each cat\n",
    "val_samples = []\n",
    "for cat_id, images in cat_to_images.items():\n",
    "    if len(images) >= 2:  # Need at least 2 images for train/val split\n",
    "        # Take 20% for validation\n",
    "        val_size = max(1, int(len(images) * 0.2))\n",
    "        val_images = random.sample(images, val_size)\n",
    "        for img_path in val_images:\n",
    "            val_samples.append((cat_id, os.path.join(TRAIN_DIR, img_path)))\n",
    "\n",
    "# Create validation dataset with the sampled images\n",
    "class ValidationDataset(Dataset):\n",
    "    def __init__(self, samples, transform=None):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        cat_id, img_path = self.samples[idx]\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except:\n",
    "            # If image can't be loaded, create a blank image\n",
    "            img = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, idx, cat_id\n",
    "\n",
    "val_dataset = ValidationDataset(val_samples, transform=val_transform)\n",
    "print(f\"Created validation dataset with {len(val_dataset)} images from {len(set([s[0] for s in val_samples]))} cats\")\n",
    "\n",
    "# Model setup\n",
    "class TripletNetwork(nn.Module):\n",
    "    def __init__(self, embedding_dim=512, pretrained=True):\n",
    "        super(TripletNetwork, self).__init__()\n",
    "        \n",
    "        # Use EfficientNet as backbone (smaller and faster than ResNet)\n",
    "        # Fix the deprecated 'pretrained' parameter with 'weights'\n",
    "        if pretrained:\n",
    "            self.backbone = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "        else:\n",
    "            self.backbone = models.efficientnet_b0(weights=None)\n",
    "        \n",
    "        # Replace classifier with embedding layer\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Identity()\n",
    "        \n",
    "        # Add an embedding layer\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Linear(in_features, embedding_dim),\n",
    "            nn.BatchNorm1d(embedding_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        embedding = self.embedding(features)\n",
    "        # L2 normalize embeddings\n",
    "        embedding = nn.functional.normalize(embedding, p=2, dim=1)\n",
    "        return embedding\n",
    "\n",
    "# Triplet loss\n",
    "class TripletLoss(nn.Module):\n",
    "    def __init__(self, margin=0.2):\n",
    "        super(TripletLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "    \n",
    "    def forward(self, anchor, positive, negative):\n",
    "        pos_dist = torch.sum((anchor - positive) ** 2, dim=1)\n",
    "        neg_dist = torch.sum((anchor - negative) ** 2, dim=1)\n",
    "        loss = torch.clamp(pos_dist - neg_dist + self.margin, min=0.0)\n",
    "        return loss.mean(), pos_dist.mean(), neg_dist.mean()\n",
    "\n",
    "# Functions for training and evaluation\n",
    "def extract_embeddings_triplet(model, dataloader, device):\n",
    "    model.eval()\n",
    "    embeddings_list = []\n",
    "    cat_ids_list = []\n",
    "    filenames_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Extracting embeddings\"):\n",
    "            images = batch[0]\n",
    "            cat_ids = batch[2]  # The third element should be cat_ids\n",
    "            \n",
    "            # Check if cat_ids is a tuple or list and extract accordingly\n",
    "            if isinstance(cat_ids, (list, tuple)) and isinstance(cat_ids[0], (tuple, list)):\n",
    "                # If cat_ids is a tuple, the third element is filename\n",
    "                filenames = [cid[2] for cid in cat_ids]\n",
    "                cat_ids = [cid[0] for cid in cat_ids]\n",
    "            else:\n",
    "                filenames = cat_ids  # For test images, cat_id is the filename\n",
    "                \n",
    "            images = images.to(device)\n",
    "            \n",
    "            # Fix autocast usage to match updated PyTorch API format\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "                embeddings = model(images)\n",
    "            \n",
    "            embeddings_list.append(embeddings.cpu().numpy())\n",
    "            cat_ids_list.extend(cat_ids)\n",
    "            filenames_list.extend(filenames)\n",
    "    \n",
    "    embeddings = np.vstack(embeddings_list)\n",
    "    return embeddings, cat_ids_list, filenames_list\n",
    "\n",
    "# Initialize model\n",
    "embedding_dim = 512\n",
    "model = TripletNetwork(embedding_dim=embedding_dim, pretrained=True)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Initialize optimizer, loss, and learning rate scheduler\n",
    "batch_size = 32\n",
    "num_workers = 2\n",
    "triplet_loss = TripletLoss(margin=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2025-04-20T17:26:50.840382Z",
     "iopub.status.busy": "2025-04-20T17:26:50.840060Z",
     "iopub.status.idle": "2025-04-20T17:47:07.696837Z",
     "shell.execute_reply": "2025-04-20T17:47:07.696073Z",
     "shell.execute_reply.started": "2025-04-20T17:26:50.840356Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import time\n",
    "from torch.amp import GradScaler\n",
    "\n",
    "# Training function with progress updates\n",
    "def train_epoch(model, train_loader, optimizer, criterion, device, scaler=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    pos_dist_sum = 0.0\n",
    "    neg_dist_sum = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    print(f\"  Starting training... ({len(train_loader)} batches)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for batch_idx, batch in enumerate(tqdm(train_loader, desc=\"Training\")):\n",
    "        # Get triplet\n",
    "        images = batch[0]\n",
    "        batch_size = len(images) // 3  # Each triplet has 3 images\n",
    "        \n",
    "        # Split batch into anchor, positive, negative\n",
    "        anchor = images[:batch_size].to(device)\n",
    "        positive = images[batch_size:2*batch_size].to(device)\n",
    "        negative = images[2*batch_size:3*batch_size].to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass with mixed precision\n",
    "        with torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "            anchor_embed = model(anchor)\n",
    "            positive_embed = model(positive)\n",
    "            negative_embed = model(negative)\n",
    "            \n",
    "            loss, pos_dist, neg_dist = criterion(anchor_embed, positive_embed, negative_embed)\n",
    "        \n",
    "        # Backward pass with gradient scaling\n",
    "        if scaler:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * batch_size\n",
    "        pos_dist_sum += pos_dist.item() * batch_size\n",
    "        neg_dist_sum += neg_dist.item() * batch_size\n",
    "        count += batch_size\n",
    "    \n",
    "    # Calculate averages with safe division\n",
    "    epoch_loss = running_loss / max(1, count)\n",
    "    avg_pos_dist = pos_dist_sum / max(1, count)\n",
    "    avg_neg_dist = neg_dist_sum / max(1, count)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  Training completed in {elapsed_time:.1f} seconds\")\n",
    "    \n",
    "    return epoch_loss, avg_pos_dist, avg_neg_dist\n",
    "\n",
    "# Validation function with progress updates\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    pos_dist_sum = 0.0\n",
    "    neg_dist_sum = 0.0\n",
    "    count = 0\n",
    "    \n",
    "    # Check if validation loader has batches\n",
    "    if len(val_loader) == 0:\n",
    "        print(\"  Warning: Validation loader is empty! Returning default values.\")\n",
    "        return 0.0, 0.0, 0.0, 0.0\n",
    "    \n",
    "    print(f\"  Starting validation... ({len(val_loader)} batches)\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "            # Check if batch is valid\n",
    "            if len(batch[0]) < 3:\n",
    "                print(f\"  Warning: Batch size too small ({len(batch[0])}), skipping.\")\n",
    "                continue\n",
    "                \n",
    "            images = batch[0]\n",
    "            batch_size = len(images) // 3  # Each triplet has 3 images\n",
    "            \n",
    "            if batch_size == 0:\n",
    "                print(\"  Warning: Batch size is zero after division, skipping.\")\n",
    "                continue\n",
    "                \n",
    "            # Split batch into anchor, positive, negative\n",
    "            anchor = images[:batch_size].to(device)\n",
    "            positive = images[batch_size:2*batch_size].to(device)\n",
    "            negative = images[2*batch_size:3*batch_size].to(device)\n",
    "            \n",
    "            # Forward pass with mixed precision\n",
    "            with torch.amp.autocast(device_type='cuda', enabled=torch.cuda.is_available()):\n",
    "                anchor_embed = model(anchor)\n",
    "                positive_embed = model(positive)\n",
    "                negative_embed = model(negative)\n",
    "                \n",
    "                loss, pos_dist, neg_dist = criterion(anchor_embed, positive_embed, negative_embed)\n",
    "            \n",
    "            # Update statistics\n",
    "            running_loss += loss.item() * batch_size\n",
    "            pos_dist_sum += pos_dist.item() * batch_size\n",
    "            neg_dist_sum += neg_dist.item() * batch_size\n",
    "            count += batch_size\n",
    "    \n",
    "    # Calculate averages with safe division\n",
    "    if count == 0:\n",
    "        print(\"  Warning: No valid batches in validation! Using default values.\")\n",
    "        val_loss = 0.0\n",
    "        avg_pos_dist = 0.0\n",
    "        avg_neg_dist = 0.0\n",
    "        separation = 0.0\n",
    "    else:\n",
    "        val_loss = running_loss / count\n",
    "        avg_pos_dist = pos_dist_sum / count\n",
    "        avg_neg_dist = neg_dist_sum / count\n",
    "        separation = avg_neg_dist - avg_pos_dist\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"  Validation completed in {elapsed_time:.1f} seconds\")\n",
    "    \n",
    "    return val_loss, separation, avg_pos_dist, avg_neg_dist\n",
    "\n",
    "# Create dataloaders with triplet sampling\n",
    "print(\"Creating data loaders...\")\n",
    "train_sampler = TripletBatchSampler(train_dataset, batch_size*3)  # *3 because we need triplets\n",
    "train_loader = DataLoader(train_dataset, batch_sampler=train_sampler, num_workers=num_workers)\n",
    "print(f\"Created training loader with {len(train_loader)} batches\")\n",
    "\n",
    "# Check if validation dataset has enough images per cat for triplet creation\n",
    "print(\"Checking validation dataset...\")\n",
    "val_cats = {}\n",
    "for cat_id, _, _ in val_dataset:\n",
    "    if cat_id not in val_cats:\n",
    "        val_cats[cat_id] = 0\n",
    "    val_cats[cat_id] += 1\n",
    "\n",
    "# Filter cats that have at least 2 images (needed for positive pairs)\n",
    "valid_val_cats = [cat_id for cat_id, count in val_cats.items() if count >= 2]\n",
    "print(f\"Found {len(valid_val_cats)} cats with at least 2 images (from {len(val_cats)} total cats)\")\n",
    "\n",
    "if len(valid_val_cats) < 2:\n",
    "    print(\"WARNING: Not enough cats with sufficient images for validation. Using a subset of training data.\")\n",
    "    # Use a subset of training data for validation\n",
    "    val_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size*3, \n",
    "        sampler=torch.utils.data.RandomSampler(train_dataset, num_samples=min(1000, len(train_dataset))),\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "else:\n",
    "    val_sampler = TripletBatchSampler(val_dataset, batch_size*3)\n",
    "    val_loader = DataLoader(val_dataset, batch_sampler=val_sampler, num_workers=num_workers)\n",
    "\n",
    "print(f\"Created validation loader with {len(val_loader)} batches\")\n",
    "\n",
    "# Training parameters\n",
    "print(\"Setting up training parameters...\")\n",
    "initial_lr = 0.0001\n",
    "fine_tuning_lr = 0.00002  # Much lower learning rate for fine-tuning (5x smaller)\n",
    "optimizer = optim.Adam(model.parameters(), lr=initial_lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1)\n",
    "scaler = GradScaler() if torch.cuda.is_available() else None\n",
    "best_separation = -float('inf')\n",
    "num_epochs_phase1 = 3  # Increased from 2 to 3\n",
    "num_epochs_phase2 = 4  # Increased from 3 to 4\n",
    "\n",
    "# Phase 1: Train with frozen backbone\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Phase 1: Training with frozen backbone\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"Model backbone: EfficientNet-B0\")\n",
    "\n",
    "print(\"Freezing backbone parameters...\")\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Show GPU memory stats before training\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated(0)/1e9:.2f} GB\")\n",
    "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved(0)/1e9:.2f} GB\")\n",
    "\n",
    "for epoch in range(1, num_epochs_phase1 + 1):\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(f\"Epoch {epoch}/{num_epochs_phase1}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    train_loss, train_pos_dist, train_neg_dist = train_epoch(model, train_loader, optimizer, triplet_loss, device, scaler)\n",
    "    val_loss, separation, val_pos_dist, val_neg_dist = validate(model, val_loader, triplet_loss, device)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Separation: {separation:.4f}\")\n",
    "    print(f\"  Avg Pos Dist: {val_pos_dist:.4f}, Avg Neg Dist: {val_neg_dist:.4f}\")\n",
    "    print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(separation)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    if new_lr != old_lr:\n",
    "        print(f\"  Learning rate decreased: {old_lr:.6f} → {new_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if separation > best_separation:\n",
    "        best_separation = separation\n",
    "        torch.save(model.state_dict(), 'best_model_phase1.pth')\n",
    "        print(f\"  New best model saved! Separation: {best_separation:.4f}\")\n",
    "\n",
    "print(f\"Phase 1 completed with best separation: {best_separation:.4f}\")\n",
    "\n",
    "# Phase 2: Fine-tune the entire model with proper settings\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Phase 2: Fine-tuning the entire model with reduced learning rate\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Load best model from Phase 1\n",
    "print(\"Loading best model from Phase 1...\")\n",
    "model.load_state_dict(torch.load('best_model_phase1.pth'))\n",
    "\n",
    "print(\"Unfreezing backbone parameters...\")\n",
    "for param in model.backbone.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Create new optimizer with reduced learning rate for Phase 2\n",
    "print(f\"Setting fine-tuning learning rate to {fine_tuning_lr:.6f} (from {initial_lr:.6f})\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=fine_tuning_lr)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1)\n",
    "\n",
    "# Reset best separation for phase 2\n",
    "best_separation_phase2 = -float('inf')\n",
    "\n",
    "for epoch in range(1, num_epochs_phase2 + 1):\n",
    "    print(\"\\n\" + \"-\"*50)\n",
    "    print(f\"Epoch {epoch}/{num_epochs_phase2}\")\n",
    "    print(\"-\"*50)\n",
    "    \n",
    "    train_loss, train_pos_dist, train_neg_dist = train_epoch(model, train_loader, optimizer, triplet_loss, device, scaler)\n",
    "    val_loss, separation, val_pos_dist, val_neg_dist = validate(model, val_loader, triplet_loss, device)\n",
    "    \n",
    "    print(\"\\nResults:\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Separation: {separation:.4f}\")\n",
    "    print(f\"  Avg Pos Dist: {val_pos_dist:.4f}, Avg Neg Dist: {val_neg_dist:.4f}\")\n",
    "    print(f\"  Current LR: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "    \n",
    "    # Update learning rate\n",
    "    old_lr = optimizer.param_groups[0]['lr']\n",
    "    scheduler.step(separation)\n",
    "    new_lr = optimizer.param_groups[0]['lr']\n",
    "    \n",
    "    if new_lr != old_lr:\n",
    "        print(f\"  Learning rate decreased: {old_lr:.6f} → {new_lr:.6f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if separation > best_separation_phase2:\n",
    "        best_separation_phase2 = separation\n",
    "        torch.save(model.state_dict(), 'best_model_phase2.pth')\n",
    "        print(f\"  New best model saved! Separation: {best_separation_phase2:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"Training complete!\")\n",
    "print(f\"Phase 1 best separation: {best_separation:.4f}\")\n",
    "print(f\"Phase 2 best separation: {best_separation_phase2:.4f}\")\n",
    "print(f\"Overall best model: {'Phase 1' if best_separation > best_separation_phase2 else 'Phase 2'}\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T17:51:32.960999Z",
     "iopub.status.busy": "2025-04-20T17:51:32.960650Z",
     "iopub.status.idle": "2025-04-20T17:53:27.637583Z",
     "shell.execute_reply": "2025-04-20T17:53:27.636647Z",
     "shell.execute_reply.started": "2025-04-20T17:51:32.960969Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the best model\n",
    "if os.path.exists('best_model_phase2.pth'):\n",
    "    model.load_state_dict(torch.load('best_model_phase2.pth', weights_only=True))\n",
    "    print(\"Loaded best model from phase 2\")\n",
    "elif os.path.exists('best_model_phase1.pth'):\n",
    "    model.load_state_dict(torch.load('best_model_phase1.pth', weights_only=True))\n",
    "    print(\"Loaded best model from phase 1\")\n",
    "else:\n",
    "    print(\"No saved model found, using the last trained model\")\n",
    "\n",
    "# Create a regular dataloader for all training images\n",
    "all_train_dataset = CatFaceDataset(TRAIN_DIR, cat_ids=sampled_cat_ids, transform=val_transform)\n",
    "all_train_loader = DataLoader(all_train_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Extract embeddings for all training images\n",
    "print(\"Extracting embeddings for all training images...\")\n",
    "train_embeddings, train_cat_ids, _ = extract_embeddings_triplet(model, all_train_loader, device)\n",
    "print(f\"Extracted training embeddings shape: {train_embeddings.shape}\")\n",
    "\n",
    "# Normalize embeddings for FAISS\n",
    "train_embeddings = train_embeddings.astype(np.float32)\n",
    "faiss.normalize_L2(train_embeddings)\n",
    "\n",
    "# Create FAISS index\n",
    "print(\"Creating FAISS index...\")\n",
    "index = faiss.IndexFlatIP(train_embeddings.shape[1])\n",
    "index.add(train_embeddings)\n",
    "print(\"FAISS index created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T18:08:58.870018Z",
     "iopub.status.busy": "2025-04-20T18:08:58.869148Z",
     "iopub.status.idle": "2025-04-20T18:16:06.365243Z",
     "shell.execute_reply": "2025-04-20T18:16:06.364368Z",
     "shell.execute_reply.started": "2025-04-20T18:08:58.869974Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load and test images from the validation directory\n",
    "VAL_DIR = '/kaggle/input/tammathon-task-1/val/val'\n",
    "print(f\"Validation directory: {VAL_DIR}\")\n",
    "print(f\"Does validation directory exist? {os.path.exists(VAL_DIR)}\")\n",
    "\n",
    "# Create a dataset for the official validation images\n",
    "class OfficialValDataset(Dataset):\n",
    "    def __init__(self, directory, transform=None):\n",
    "        self.directory = directory\n",
    "        self.transform = transform\n",
    "        self.image_files = []\n",
    "        self.cat_ids = []\n",
    "        \n",
    "        # Get all image files and their cat IDs from directory structure\n",
    "        for cat_id in os.listdir(directory):\n",
    "            cat_dir = os.path.join(directory, cat_id)\n",
    "            if os.path.isdir(cat_dir):\n",
    "                for img_file in os.listdir(cat_dir):\n",
    "                    if img_file.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        self.image_files.append(os.path.join(cat_dir, img_file))\n",
    "                        self.cat_ids.append(cat_id)\n",
    "        \n",
    "        print(f\"Found {len(self.image_files)} validation images across {len(set(self.cat_ids))} cat classes\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        cat_id = self.cat_ids[idx]\n",
    "        filename = os.path.basename(img_path)\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            img = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, idx, cat_id\n",
    "\n",
    "# Create official validation dataset and dataloader\n",
    "if os.path.exists(VAL_DIR):\n",
    "    official_val_dataset = OfficialValDataset(VAL_DIR, transform=val_transform)\n",
    "    official_val_loader = DataLoader(official_val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    # Extract embeddings for official validation images\n",
    "    print(\"Extracting embeddings for official validation images...\")\n",
    "    val_embeddings, val_true_cat_ids, _ = extract_embeddings_triplet(model, official_val_loader, device)\n",
    "    print(f\"Extracted validation embeddings shape: {val_embeddings.shape}\")\n",
    "\n",
    "    # Normalize validation embeddings\n",
    "    val_embeddings = val_embeddings.astype(np.float32)\n",
    "    faiss.normalize_L2(val_embeddings)\n",
    "\n",
    "    # Calculate overall accuracy on validation set\n",
    "    print(\"Finding top matches for validation images...\")\n",
    "    D, I = index.search(val_embeddings, k=3)\n",
    "\n",
    "    correct_top1 = 0\n",
    "    correct_top3 = 0\n",
    "    for i, true_cat_id in enumerate(val_true_cat_ids):\n",
    "        top3_indices = I[i]\n",
    "        top3_cat_ids = [train_cat_ids[idx] for idx in top3_indices]\n",
    "        \n",
    "        if true_cat_id == top3_cat_ids[0]:\n",
    "            correct_top1 += 1\n",
    "        if true_cat_id in top3_cat_ids:\n",
    "            correct_top3 += 1\n",
    "\n",
    "    total_val = len(val_true_cat_ids)\n",
    "    top1_accuracy = correct_top1 / total_val\n",
    "    top3_accuracy = correct_top3 / total_val\n",
    "\n",
    "    print(f\"Official Validation Results:\")\n",
    "    print(f\"Top-1 Accuracy: {top1_accuracy:.4f} ({correct_top1}/{total_val})\")\n",
    "    print(f\"Top-3 Accuracy: {top3_accuracy:.4f} ({correct_top3}/{total_val})\")\n",
    "\n",
    "    # Visualize validation results for random examples\n",
    "    num_val_examples = 5\n",
    "    indices = random.sample(range(len(val_embeddings)), min(num_val_examples, len(val_embeddings)))\n",
    "\n",
    "    plt.figure(figsize=(20, 4 * num_val_examples))\n",
    "\n",
    "    for plot_idx, idx in enumerate(indices):\n",
    "        query_embedding = val_embeddings[idx:idx+1]\n",
    "        true_cat_id = val_true_cat_ids[idx]\n",
    "        \n",
    "        # Find the image path from the dataset\n",
    "        query_img_path = official_val_dataset.image_files[idx]\n",
    "        \n",
    "        # Search for similar cats\n",
    "        D, I = index.search(query_embedding, k=5)\n",
    "        \n",
    "        # Get top cat IDs\n",
    "        top_indices = I[0]\n",
    "        top_cat_ids = [train_cat_ids[idx] for idx in top_indices]\n",
    "        top_scores = D[0]\n",
    "        \n",
    "        # Display query image\n",
    "        plt.subplot(num_val_examples, 6, plot_idx*6 + 1)\n",
    "        query_img = Image.open(query_img_path).convert('RGB')\n",
    "        plt.imshow(query_img)\n",
    "        plt.title(f\"Query: {true_cat_id}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Display top 5 predicted cat images\n",
    "        for j in range(5):\n",
    "            plt.subplot(num_val_examples, 6, plot_idx*6 + j + 2)\n",
    "            pred_cat_id = top_cat_ids[j]\n",
    "            pred_path = find_image_by_cat_id(pred_cat_id)\n",
    "            similarity = top_scores[j]\n",
    "            is_correct = (pred_cat_id == true_cat_id)\n",
    "            \n",
    "            if pred_path:\n",
    "                try:\n",
    "                    pred_img = Image.open(pred_path).convert('RGB')\n",
    "                    plt.imshow(pred_img)\n",
    "                    \n",
    "                    # Add colored border to indicate correct/incorrect\n",
    "                    border_color = 'green' if is_correct else 'red'\n",
    "                    plt.gca().add_patch(Rectangle((0, 0), 1, 1, fill=False, edgecolor=border_color, lw=5, \n",
    "                                                transform=plt.gca().transAxes))\n",
    "                except Exception as e:\n",
    "                    plt.text(0.5, 0.5, \"Error loading image\", ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            else:\n",
    "                plt.text(0.5, 0.5, f\"No image for\\n{pred_cat_id}\", ha='center', va='center', transform=plt.gca().transAxes)\n",
    "            \n",
    "            plt.title(f\"Pred {j+1}: {pred_cat_id}\\nScore: {similarity:.3f}\\n{'✓' if is_correct else '✗'}\")\n",
    "            plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Validation directory not found. Skipping validation accuracy calculation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-20T18:29:26.201490Z",
     "iopub.status.busy": "2025-04-20T18:29:26.200891Z",
     "iopub.status.idle": "2025-04-20T18:40:41.629912Z",
     "shell.execute_reply": "2025-04-20T18:40:41.629033Z",
     "shell.execute_reply.started": "2025-04-20T18:29:26.201467Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import torch\n",
    "import faiss\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# Load the CSV file\n",
    "csv_path = '/kaggle/input/tammathon-task-1/train.csv'\n",
    "print(f\"Loading CSV from: {csv_path}\")\n",
    "\n",
    "# Load the CSV and filter to include only the first 10,000 classes\n",
    "df = pd.read_csv(csv_path)\n",
    "print(f\"CSV loaded with {len(df)} total entries\")\n",
    "\n",
    "# Filter to only include first 10,000 classes\n",
    "df['label'] = df['label'].astype(int)  # Ensure label is integer\n",
    "filtered_df = df[df['label'] < 10000]\n",
    "print(f\"Filtered to {len(filtered_df)} entries with labels < 10000\")\n",
    "\n",
    "# Save filtered CSV temporarily\n",
    "filtered_csv_path = '/kaggle/working/filtered_train.csv'\n",
    "filtered_df.to_csv(filtered_csv_path, index=False)\n",
    "\n",
    "# Create a dataset for the filtered CSV images\n",
    "class CSVImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, base_dir='/kaggle/input/tammathon-task-1', transform=None):\n",
    "        self.df = pd.read_csv(csv_file)\n",
    "        self.base_dir = base_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Clean up and validate data\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        self.filenames = []\n",
    "        \n",
    "        for idx, row in self.df.iterrows():\n",
    "            try:\n",
    "                filename = row['filename']\n",
    "                label = row['label']\n",
    "                \n",
    "                # Construct full path (try different possible paths)\n",
    "                img_path = None\n",
    "                possible_paths = [\n",
    "                    os.path.join(self.base_dir, filename),\n",
    "                    os.path.join(self.base_dir, 'train', filename),\n",
    "                    filename if os.path.isabs(filename) else os.path.join(self.base_dir, filename)\n",
    "                ]\n",
    "                \n",
    "                for path in possible_paths:\n",
    "                    if os.path.exists(path):\n",
    "                        img_path = path\n",
    "                        break\n",
    "                \n",
    "                if img_path:\n",
    "                    self.image_paths.append(img_path)\n",
    "                    self.labels.append(label)\n",
    "                    self.filenames.append(filename)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing row {idx}: {e}\")\n",
    "        \n",
    "        print(f\"Found {len(self.image_paths)} valid images out of {len(self.df)} entries\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        filename = self.filenames[idx]\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_path}: {e}\")\n",
    "            img = Image.new('RGB', (224, 224), color='gray')\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, idx, str(label)  # Return as (image, index, label)\n",
    "\n",
    "# Create dataset and dataloader for filtered CSV images\n",
    "csv_dataset = CSVImageDataset(filtered_csv_path, transform=val_transform)\n",
    "csv_loader = DataLoader(csv_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "# Load the best model\n",
    "print(\"Loading best model...\")\n",
    "best_model_path = 'best_model_phase2.pth'  # Use Phase 2 model if it's better, otherwise use Phase 1\n",
    "if not os.path.exists(best_model_path):\n",
    "    best_model_path = 'best_model_phase1.pth'\n",
    "    \n",
    "model.load_state_dict(torch.load(best_model_path, weights_only=True))\n",
    "model.eval()\n",
    "\n",
    "# Extract embeddings for filtered CSV images\n",
    "print(\"Extracting embeddings for CSV images (first 10,000 classes only)...\")\n",
    "csv_embeddings, csv_indices, csv_labels = extract_embeddings_triplet(model, csv_loader, device)\n",
    "print(f\"Extracted {len(csv_embeddings)} embeddings\")\n",
    "\n",
    "# Normalize embeddings\n",
    "csv_embeddings = csv_embeddings.astype(np.float32)\n",
    "faiss.normalize_L2(csv_embeddings)\n",
    "\n",
    "# Create index for searching\n",
    "print(\"Creating FAISS index...\")\n",
    "dimension = csv_embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)\n",
    "index.add(csv_embeddings)\n",
    "\n",
    "# Evaluate model predictions\n",
    "print(\"Evaluating model predictions...\")\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "correct_examples = []\n",
    "\n",
    "# Group images by label\n",
    "label_to_indices = {}\n",
    "for i, label in enumerate(csv_labels):\n",
    "    if label not in label_to_indices:\n",
    "        label_to_indices[label] = []\n",
    "    label_to_indices[label].append(i)\n",
    "\n",
    "# Evaluate each image\n",
    "for idx, label in enumerate(tqdm(csv_labels, desc=\"Evaluating\")):\n",
    "    # Skip if this label has only one image\n",
    "    if len(label_to_indices[label]) <= 1:\n",
    "        continue\n",
    "    \n",
    "    # Use current image as query\n",
    "    query_embedding = csv_embeddings[idx:idx+1]\n",
    "    query_path = csv_dataset.image_paths[idx]\n",
    "    query_label = label\n",
    "    \n",
    "    # Search for similar images (k=2 to include the query image itself)\n",
    "    D, I = index.search(query_embedding, k=2)\n",
    "    \n",
    "    # Get top match (excluding self)\n",
    "    top_match_idx = I[0][1] if I[0][0] == idx else I[0][0]\n",
    "    predicted_label = csv_labels[top_match_idx]\n",
    "    \n",
    "    # Check if prediction is correct\n",
    "    is_correct = (predicted_label == query_label)\n",
    "    \n",
    "    if is_correct:\n",
    "        correct_predictions += 1\n",
    "        # Store example if it's correct\n",
    "        correct_examples.append({\n",
    "            'query_path': query_path,\n",
    "            'query_label': query_label,\n",
    "            'pred_path': csv_dataset.image_paths[top_match_idx],\n",
    "            'pred_label': predicted_label,\n",
    "            'score': D[0][1] if I[0][0] == idx else D[0][0]\n",
    "        })\n",
    "    \n",
    "    total_predictions += 1\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
    "print(f\"\\nModel Performance Overview (First 10,000 Classes Only):\")\n",
    "print(f\"Total predictions: {total_predictions}\")\n",
    "print(f\"Correct predictions: {correct_predictions}\")\n",
    "print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "# Display 10 examples of correct predictions\n",
    "if correct_examples:\n",
    "    # Sort by confidence (score)\n",
    "    correct_examples.sort(key=lambda x: x['score'], reverse=True)\n",
    "    \n",
    "    # Select 10 random examples from the top 100 to show diversity\n",
    "    top_examples = correct_examples[:100]\n",
    "    examples_to_show = random.sample(top_examples, min(10, len(top_examples)))\n",
    "    \n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(10, 2, figsize=(12, 25))\n",
    "    fig.suptitle('Examples of Correct Predictions (Query: Prediction)', fontsize=16)\n",
    "    \n",
    "    for i, example in enumerate(examples_to_show):\n",
    "        if i >= 10:\n",
    "            break\n",
    "            \n",
    "        # Display query image\n",
    "        try:\n",
    "            query_img = Image.open(example['query_path']).convert('RGB')\n",
    "            axes[i, 0].imshow(query_img)\n",
    "            axes[i, 0].set_title(f\"Query: Label {example['query_label']}\")\n",
    "            axes[i, 0].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[i, 0].text(0.5, 0.5, \"Error loading image\", ha='center', va='center')\n",
    "            axes[i, 0].axis('off')\n",
    "        \n",
    "        # Display predicted image\n",
    "        try:\n",
    "            pred_img = Image.open(example['pred_path']).convert('RGB')\n",
    "            axes[i, 1].imshow(pred_img)\n",
    "            axes[i, 1].set_title(f\"Prediction: Label {example['pred_label']}\\nScore: {example['score']:.3f}\")\n",
    "            axes[i, 1].axis('off')\n",
    "        except Exception as e:\n",
    "            axes[i, 1].text(0.5, 0.5, \"Error loading image\", ha='center', va='center')\n",
    "            axes[i, 1].axis('off')\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No correct predictions to display.\")\n",
    "\n",
    "print(\"Evaluation complete!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11858416,
     "sourceId": 95687,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
